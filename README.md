<div class="Box-sc-g0xbh4-0 bJMeLZ js-snippet-clipboard-copy-unpositioned" data-hpc="true"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bf00e4154552d507a39364381e5d48ff28b86c34921cf59adb137db0fa10988d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d61696e7461696e65642533462d5945532d677265656e2e737667"><img src="https://camo.githubusercontent.com/bf00e4154552d507a39364381e5d48ff28b86c34921cf59adb137db0fa10988d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d61696e7461696e65642533462d5945532d677265656e2e737667" alt="维护" data-canonical-src="https://img.shields.io/badge/Maintained%3F-YES-green.svg" style="max-width: 100%;"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8302b4d15233a68cdc85b88c77da45c986b8086926177dfa76b65679dc8b1366/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f52656c656173652d50524f442d79656c6c6f772e737667"><img src="https://camo.githubusercontent.com/8302b4d15233a68cdc85b88c77da45c986b8086926177dfa76b65679dc8b1366/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f52656c656173652d50524f442d79656c6c6f772e737667" alt="GitHub" data-canonical-src="https://img.shields.io/badge/Release-PROD-yellow.svg" style="max-width: 100%;"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1f25102f0ebd599227866497495e598b22049f0de8ea945c8eeb632473e81846/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c616e6775616765732d4d554c54492d626c75652e737667"><img src="https://camo.githubusercontent.com/1f25102f0ebd599227866497495e598b22049f0de8ea945c8eeb632473e81846/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c616e6775616765732d4d554c54492d626c75652e737667" alt="GitHub" data-canonical-src="https://img.shields.io/badge/Languages-MULTI-blue.svg" style="max-width: 100%;"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d952b801bcd9ac988caa23ba728f9b5af077cbb229f86b1f4639039b19500304/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d6c69676874677265792e737667"><img src="https://camo.githubusercontent.com/d952b801bcd9ac988caa23ba728f9b5af077cbb229f86b1f4639039b19500304/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d6c69676874677265792e737667" alt="GitHub" data-canonical-src="https://img.shields.io/badge/License-MIT-lightgrey.svg" style="max-width: 100%;"></a></p>
<div class="markdown-heading" dir="auto"><h1 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">计算机视觉预训练模型</font></font></h1><a id="user-content-computer-vision-pretrained-models" class="anchor-element" aria-label="永久链接：计算机视觉预训练模型" href="#computer-vision-pretrained-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/balavenkatesh3322/CV-pretrained-model/blob/master/logo.jpg"><img src="https://github.com/balavenkatesh3322/CV-pretrained-model/raw/master/logo.jpg" alt="简历标志" style="max-width: 100%;"></a></p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">什么是预训练模型？</font></font></h2><a id="user-content-what-is-pre-trained-model" class="anchor-element" aria-label="永久链接：什么是预训练模型？" href="#what-is-pre-trained-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">预训练模型是其他人为解决类似问题而创建的模型。</font><font style="vertical-align: inherit;">我们可以使用针对其他问题训练的模型作为起点，而不是从头开始构建模型来解决类似问题。</font><font style="vertical-align: inherit;">预训练模型在您的应用程序中可能不是 100% 准确。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">例如，如果你想建造一辆自学习汽车。</font><font style="vertical-align: inherit;">您可以花费数年时间从头开始构建一个像样的图像识别算法，也可以采用 Google 的初始模型（预先训练的模型），该模型基于</font></font><a href="http://www.image-net.org/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ImageNet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">数据构建来识别这些图片中的图像。</font></font></p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">其他预训练模型</font></font></h2><a id="user-content-other-pre-trained-models" class="anchor-element" aria-label="永久链接：其他预训练模型" href="#other-pre-trained-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><a href="https://github.com/balavenkatesh3322/NLP-pretrained-model"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NLP 预训练模型</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font></li>
<li><a href="https://github.com/balavenkatesh3322/audio-pretrained-model"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">音频和语音预训练模型</font></font></a></li>
</ul>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">模型部署库</font></font></h2><a id="user-content-model-deployment-library" class="anchor-element" aria-label="永久链接：模型部署库" href="#model-deployment-library"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><a href="https://github.com/balavenkatesh3322/model_deployment"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">模特服务</font></font></a></li>
</ul>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">框架</font></font></h3><a id="user-content-framework" class="anchor-element" aria-label="永久链接：框架" href="#framework"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><a href="#tensorflow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">张量流</font></font></a></li>
<li><a href="#keras"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">喀拉斯</font></font></a></li>
<li><a href="#pytorch"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">火炬</font></font></a></li>
<li><a href="#caffe"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">咖啡</font></font></a></li>
<li><a href="#mxnet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MXNet</font></font></a></li>
</ul>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">模型可视化</font></font></h3><a id="user-content-model-visualization" class="anchor-element" aria-label="永久链接：模型可视化" href="#model-visualization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">您可以使用</font></font><a href="https://github.com/lutzroeder/Netron"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Netron</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">查看每个模型的网络架构的可视化。</font></font></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/balavenkatesh3322/CV-pretrained-model/blob/master/netron.png"><img src="https://github.com/balavenkatesh3322/CV-pretrained-model/raw/master/netron.png" alt="简历标志" style="max-width: 100%;"></a></p>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">张量流</font></font><a name="user-content-tensorflow"></a></h3><a id="user-content-tensorflow-" class="anchor-element" aria-label="永久链接：张量流" href="#tensorflow-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<table>
<thead>
<tr>
<th align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">型号名称</font></font></th>
<th align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">描述</font></font></th>
<th align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">框架</font></font></th>
<th align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">执照</font></font></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://github.com/tensorflow/models/tree/master/research/object_detection"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">物体检测</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">定位和识别单个图像中的多个对象。</font></font></td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/tensorflow/models/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">阿帕奇许可证</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/matterport/Mask_RCNN"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mask R-CNN</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">该模型为图像中对象的每个实例生成边界框和分割掩模。</font><font style="vertical-align: inherit;">它基于特征金字塔网络 (FPN) 和 ResNet101 主干网。</font></font></td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/matterport/Mask_RCNN/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">麻省理工学院许可证 (MIT)</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/smallcorgi/Faster-RCNN_TF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">更快的RCNN</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">这是 Faster RCNN 的实验性 Tensorflow 实现，Faster RCNN 是一个带有区域提议网络的用于对象检测的卷积网络。</font></font></td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/smallcorgi/Faster-RCNN_TF/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">麻省理工学院许可证</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/gliese581gg/YOLO_tensorflow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">YOLO TensorFlow</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">这是 YOLO：实时对象检测的张量流实现。</font></font></td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/gliese581gg/YOLO_tensorflow/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">风俗</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/thtrieu/darkflow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">YOLO TensorFlow ++</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">“YOLO：实时对象检测”的 TensorFlow 实现，经过训练并实际支持在移动设备上实时运行。</font></font></td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/thtrieu/darkflow/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GNU 通用公共许可证</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">移动网络</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MobileNet 在延迟、大小和准确性之间进行权衡，同时与文献中的流行模型进行比较。</font></font></td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/tensorflow/models/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">麻省理工学院许可证 (MIT)</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/tensorflow/models/tree/master/research/deeplab"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">深度实验室</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">用于语义图像分割的深度标记。</font></font></td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/tensorflow/models/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">阿帕奇许可证</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/pavelgonchar/colornet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">彩色网</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">神经网络对灰度图像进行着色。</font></font></td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">未找到</font></font></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/tensorlayer/srgan"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SRGAN</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用生成对抗网络的逼真单图像超分辨率。</font></font></td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">未找到</font></font></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/trailbehind/DeepOSM"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">深度OSM</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">使用 OpenStreetMap 功能和卫星图像训练 TensorFlow 神经网络。</font></font></td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/trailbehind/DeepOSM/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">麻省理工学院许可证 (MIT)</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/yunjey/domain-transfer-network"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">域名转移网络</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">无监督跨域图像生成的实现。</font></font></td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/yunjey/domain-transfer-network/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">麻省理工学院许可证</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/yunjey/show-attend-and-tell"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">展示、出席和讲述</font></font></a></td>
<td align="center">Attention Based Image Caption Generator.</td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/yunjey/show-attend-and-tell/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/natanielruiz/android-yolo">android-yolo</a></td>
<td align="center">Real-time object detection on Android using the YOLO network, powered by TensorFlow.</td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/natanielruiz/android-yolo/master/LICENSE" rel="nofollow">Apache License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/jiny2001/dcscn-super-resolutiont">DCSCN Super Resolution</a></td>
<td align="center">This is a tensorflow implementation of "Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network", a deep learning based Single-Image Super-Resolution (SISR) model.</td>
<td align="center"><code>Tensorflow</code></td>
<td align="center">Not Found</td>
</tr>
<tr>
<td align="center"><a href="https://github.com/zsdonghao/text-to-image">GAN-CLS</a></td>
<td align="center">This is an experimental tensorflow implementation of synthesizing images.</td>
<td align="center"><code>Tensorflow</code></td>
<td align="center">Not Found</td>
</tr>
<tr>
<td align="center"><a href="https://github.com/zsdonghao/u-net-brain-tumor">U-Net</a></td>
<td align="center">For Brain Tumor Segmentation.</td>
<td align="center"><code>Tensorflow</code></td>
<td align="center">Not Found</td>
</tr>
<tr>
<td align="center"><a href="https://github.com/luoxier/CycleGAN_Tensorlayer">Improved CycleGAN</a></td>
<td align="center">Unpaired Image to Image Translation.</td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/luoxier/CycleGAN_Tensorlayer/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/tensorflow/models/tree/master/research/im2txt">Im2txt</a></td>
<td align="center">Image-to-text neural network for image captioning.</td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/tensorflow/models/master/LICENSE" rel="nofollow">Apache License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/tensorflow/models/tree/master/research/slim">SLIM</a></td>
<td align="center">Image classification models in TF-Slim.</td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/tensorflow/models/master/LICENSE" rel="nofollow">Apache License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/tensorflow/models/tree/master/research/delf">DELF</a></td>
<td align="center">Deep local features for image matching and retrieval.</td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/tensorflow/models/master/LICENSE" rel="nofollow">Apache License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/tensorflow/models/tree/master/research/compression">Compression</a></td>
<td align="center">Compressing and decompressing images using a pre-trained Residual GRU network.</td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/tensorflow/models/master/LICENSE" rel="nofollow">Apache License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/tensorflow/models/tree/master/research/attention_ocr">AttentionOCR</a></td>
<td align="center">A model for real-world image text extraction.</td>
<td align="center"><code>Tensorflow</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/tensorflow/models/master/LICENSE" rel="nofollow">Apache License</a></td>
</tr>
</tbody>
</table>
<div align="right" dir="auto">
    <b></b><b><a href="#framework">↥ Back To Top</a></b>
</div>
<hr>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Keras <a name="user-content-keras"></a></h3><a id="user-content-keras-" class="anchor-element" aria-label="永久链接：Keras" href="#keras-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<table>
<thead>
<tr>
<th align="center">Model Name</th>
<th align="center">Description</th>
<th align="center">Framework</th>
<th align="center">License</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://github.com/matterport/Mask_RCNN">Mask R-CNN</a></td>
<td align="center">The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/matterport/Mask_RCNN/master/LICENSE" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py">VGG16</a></td>
<td align="center">Very Deep Convolutional Networks for Large-Scale Image Recognition.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/keras-team/keras-applications/master/LICENSE" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg19.py">VGG19</a></td>
<td align="center">Very Deep Convolutional Networks for Large-Scale Image Recognition.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/keras-team/keras-applications/master/LICENSE" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet_common.py">ResNet</a></td>
<td align="center">Deep Residual Learning for Image Recognition.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/keras-team/keras-applications/master/LICENSE" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py">ResNet50</a></td>
<td align="center">Deep Residual Learning for Image Recognition.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/keras-team/keras-applications/master/LICENSE" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/keras-team/keras-applications/blob/master/keras_applications/nasnet.py">Nasnet</a></td>
<td align="center">NASNet refers to Neural Architecture Search Network, a family of models that were designed automatically by learning the model architectures directly on the dataset of interest.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/keras-team/keras-applications/master/LICENSE" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py">MobileNet</a></td>
<td align="center">MobileNet v1 models for Keras.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/keras-team/keras-applications/master/LICENSE" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet_v2.py">MobileNet V2</a></td>
<td align="center">MobileNet v2 models for Keras.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/keras-team/keras-applications/master/LICENSE" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet_v3.py">MobileNet V3</a></td>
<td align="center">MobileNet v3 models for Keras.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/keras-team/keras-applications/master/LICENSE" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/keras-team/keras-applications/blob/master/keras_applications/efficientnet.py">efficientnet</a></td>
<td align="center">Rethinking Model Scaling for Convolutional Neural Networks.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/keras-team/keras-applications/master/LICENSE" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/awentzonline/image-analogies">Image analogies</a></td>
<td align="center">Generate image analogies using neural matching and blending.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/awentzonline/image-analogies/master/LICENSE.txt" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/divamgupta/image-segmentation-keras">Popular Image Segmentation Models</a></td>
<td align="center">Implementation of Segnet, FCN, UNet and other models in Keras.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/divamgupta/image-segmentation-keras/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/jocicmarko/ultrasound-nerve-segmentation">Ultrasound nerve segmentation</a></td>
<td align="center">This tutorial shows how to use Keras library to build deep neural network for ultrasound image nerve segmentation.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/jocicmarko/ultrasound-nerve-segmentation/master/LICENSE.md" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/abbypa/NNProject_DeepMask">DeepMask object segmentation</a></td>
<td align="center">This is a Keras-based Python implementation of DeepMask- a complex deep neural network for learning object segmentation masks.</td>
<td align="center"><code>Keras</code></td>
<td align="center">Not Found</td>
</tr>
<tr>
<td align="center"><a href="https://github.com/elliottd/GroundedTranslation">Monolingual and Multilingual Image Captioning</a></td>
<td align="center">This is the source code that accompanies Multilingual Image Description with Neural Sequence Models.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/elliottd/GroundedTranslation/master/LICENSE" rel="nofollow">BSD-3-Clause License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/pix2pix">pix2pix</a></td>
<td align="center">Keras implementation of Image-to-Image Translation with Conditional Adversarial Networks by Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A.</td>
<td align="center"><code>Keras</code></td>
<td align="center">Not Found</td>
</tr>
<tr>
<td align="center"><a href="https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/Colorful">Colorful Image colorization</a></td>
<td align="center">B&amp;W to color.</td>
<td align="center"><code>Keras</code></td>
<td align="center">Not Found</td>
</tr>
<tr>
<td align="center"><a href="https://github.com/eriklindernoren/Keras-GAN/blob/master/cyclegan/cyclegan.py">CycleGAN</a></td>
<td align="center">Implementation of <em>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</em>.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/eriklindernoren/Keras-GAN/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/eriklindernoren/Keras-GAN/blob/master/dualgan/dualgan.py">DualGAN</a></td>
<td align="center">Implementation of <em>DualGAN: Unsupervised Dual Learning for Image-to-Image Translation</em>.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/eriklindernoren/Keras-GAN/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/eriklindernoren/Keras-GAN/blob/master/srgan/srgan.py">Super-Resolution GAN</a></td>
<td align="center">Implementation of <em>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</em>.</td>
<td align="center"><code>Keras</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/eriklindernoren/Keras-GAN/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
</tbody>
</table>
<div align="right" dir="auto">
    <b></b><b><a href="#framework">↥ Back To Top</a></b>
</div>
<hr>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">PyTorch <a name="user-content-pytorch"></a></h3><a id="user-content-pytorch-" class="anchor-element" aria-label="永久链接：PyTorch" href="#pytorch-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<table>
<thead>
<tr>
<th align="center">Model Name</th>
<th align="center">Description</th>
<th align="center">Framework</th>
<th align="center">License</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://github.com/facebookresearch/detectron2">detectron2</a></td>
<td align="center">Detectron2 is Facebook AI Research's next generation software system that implements state-of-the-art object detection algorithms</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/facebookresearch/detectron2/master/LICENSE" rel="nofollow">Apache License 2.0</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/NVIDIA/FastPhotoStyle">FastPhotoStyle</a></td>
<td align="center">A Closed-form Solution to Photorealistic Image Stylization.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/NVIDIA/FastPhotoStyle/master/LICENSE.md" rel="nofollow">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public Licens</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">pytorch-CycleGAN-and-pix2pix</a></td>
<td align="center">A Closed-form Solution to Photorealistic Image Stylization.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/LICENSE" rel="nofollow">BSD License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/facebookresearch/maskrcnn-benchmark">maskrcnn-benchmark</a></td>
<td align="center">Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/facebookresearch/maskrcnn-benchmark/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/DmitryUlyanov/deep-image-prior">deep-image-prior</a></td>
<td align="center">Image restoration with neural networks but without learning.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/DmitryUlyanov/deep-image-prior/master/LICENSE" rel="nofollow">Apache License 2.0</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/yunjey/StarGAN">StarGAN</a></td>
<td align="center">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/yunjey/StarGAN/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/jwyang/faster-rcnn.pytorch">faster-rcnn.pytorch</a></td>
<td align="center">This project is a faster faster R-CNN implementation, aimed to accelerating the training of faster R-CNN object detection models.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/jwyang/faster-rcnn.pytorch/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/NVIDIA/pix2pixHD">pix2pixHD</a></td>
<td align="center">Synthesizing and manipulating 2048x1024 images with conditional GANs.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/NVIDIA/pix2pixHD/master/LICENSE.txt" rel="nofollow">BSD License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/mdbloice/Augmentor">Augmentor</a></td>
<td align="center">Image augmentation library in Python for machine learning.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/mdbloice/Augmentor/master/LICENSE.md" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/albumentations-team/albumentations">albumentations</a></td>
<td align="center">Fast image augmentation library.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/albumentations-team/albumentations/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/AKSHAYUBHAT/DeepVideoAnalytics">Deep Video Analytics</a></td>
<td align="center">Deep Video Analytics is a platform for indexing and extracting information from videos and images</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/AKSHAYUBHAT/DeepVideoAnalytics/master/LICENSE" rel="nofollow">Custom</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/CSAILVision/semantic-segmentation-pytorch">semantic-segmentation-pytorch</a></td>
<td align="center">Pytorch implementation for Semantic Segmentation/Scene Parsing on MIT ADE20K dataset.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/CSAILVision/semantic-segmentation-pytorch/master/LICENSE" rel="nofollow">BSD 3-Clause License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/bgshih/crnn">An End-to-End Trainable Neural Network for Image-based Sequence Recognition</a></td>
<td align="center">This software implements the Convolutional Recurrent Neural Network (CRNN), a combination of CNN, RNN and CTC loss for image-based sequence recognition tasks, such as scene text recognition and OCR.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/bgshih/crnn/master/LICENSE" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/mingyuliutw/UNIT">UNIT</a></td>
<td align="center">PyTorch Implementation of our Coupled VAE-GAN algorithm for Unsupervised Image-to-Image Translation.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/mingyuliutw/UNIT/master/LICENSE.md" rel="nofollow">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/jiesutd/NCRFpp">Neural Sequence labeling model</a></td>
<td align="center">Sequence labeling models are quite popular in many NLP tasks, such as Named Entity Recognition (NER), part-of-speech (POS) tagging and word segmentation.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/jiesutd/NCRFpp/master/LICENCE" rel="nofollow">Apache License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/longcw/faster_rcnn_pytorch">faster rcnn</a></td>
<td align="center">This is a PyTorch implementation of Faster RCNN. This project is mainly based on py-faster-rcnn and TFFRCNN. For details about R-CNN please refer to the paper Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks by Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/longcw/faster_rcnn_pytorch/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/ZijunDeng/pytorch-semantic-segmentation">pytorch-semantic-segmentation</a></td>
<td align="center">PyTorch for Semantic Segmentation.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/ZijunDeng/pytorch-semantic-segmentation/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/thstkdgus35/EDSR-PyTorch">EDSR-PyTorch</a></td>
<td align="center">PyTorch version of the paper 'Enhanced Deep Residual Networks for Single Image Super-Resolution'.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/thstkdgus35/EDSR-PyTorch/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/osmr/imgclsmob">image-classification-mobile</a></td>
<td align="center">Collection of classification models pretrained on the ImageNet-1K.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/osmr/imgclsmob/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/facebookresearch/FaderNetworks">FaderNetworks</a></td>
<td align="center">Fader Networks: Manipulating Images by Sliding Attributes - NIPS 2017.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/facebookresearch/FaderNetworks/master/LICENSE" rel="nofollow">Creative Commons Attribution-NonCommercial 4.0 International Public License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/ruotianluo/ImageCaptioning.pytorch">neuraltalk2-pytorch</a></td>
<td align="center">Image captioning model in pytorch (finetunable cnn in branch with_finetune).</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/ruotianluo/ImageCaptioning.pytorch/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/seungwonpark/RandWireNN">RandWireNN</a></td>
<td align="center">Implementation of: "Exploring Randomly Wired Neural Networks for Image Recognition".</td>
<td align="center"><code>PyTorch</code></td>
<td align="center">Not Found</td>
</tr>
<tr>
<td align="center"><a href="https://github.com/hanzhanggit/StackGAN-v2">stackGAN-v2</a></td>
<td align="center">Pytorch implementation for reproducing StackGAN_v2 results in the paper StackGAN++.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/hanzhanggit/StackGAN-v2/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/ignacio-rocco/detectorch">Detectron models for Object Detection</a></td>
<td align="center">This code allows to use some of the Detectron models for object detection from Facebook AI Research with PyTorch.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/ignacio-rocco/detectorch/master/LICENSE" rel="nofollow">Apache License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/scaelles/DEXTR-PyTorch">DEXTR-PyTorch</a></td>
<td align="center">This paper explores the use of extreme points in an object (left-most, right-most, top, bottom pixels) as input to obtain precise object segmentation for images and videos.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/scaelles/DEXTR-PyTorch/master/LICENSE" rel="nofollow">GNU GENERAL PUBLIC LICENSE</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/fxia22/pointnet.pytorch">pointnet.pytorch</a></td>
<td align="center">Pytorch implementation for "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/fxia22/pointnet.pytorch/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/ruotianluo/self-critical.pytorch">self-critical.pytorch</a></td>
<td align="center">This repository includes the unofficial implementation Self-critical Sequence Training for Image Captioning and Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/ruotianluo/self-critical.pytorch/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/mattmacy/vnet.pytorch">vnet.pytorch</a></td>
<td align="center">A Pytorch implementation for V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/mattmacy/vnet.pytorch/master/LICENSE" rel="nofollow">BSD 3-Clause License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/bodokaiser/piwise">piwise</a></td>
<td align="center">Pixel-wise segmentation on VOC2012 dataset using pytorch.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/bodokaiser/piwise/master/LICENSE.md" rel="nofollow">BSD 3-Clause License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/Lextal/pspnet-pytorch">pspnet-pytorch</a></td>
<td align="center">PyTorch implementation of PSPNet segmentation network.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center">Not Found</td>
</tr>
<tr>
<td align="center"><a href="https://github.com/twtygqyy/pytorch-SRResNet">pytorch-SRResNet</a></td>
<td align="center">Pytorch implementation for Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/twtygqyy/pytorch-SRResNet/master/LICENSE" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/chenxi116/PNASNet.pytorch">PNASNet.pytorch</a></td>
<td align="center">PyTorch implementation of PNASNet-5 on ImageNet.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/chenxi116/PNASNet.pytorch/master/LICENSE" rel="nofollow">Apache License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/felixgwu/img_classification_pk_pytorch">img_classification_pk_pytorch</a></td>
<td align="center">Quickly comparing your image classification models with the state-of-the-art models.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center">Not Found</td>
</tr>
<tr>
<td align="center"><a href="https://github.com/utkuozbulak/pytorch-cnn-adversarial-attacks">Deep Neural Networks are Easily Fooled</a></td>
<td align="center">High Confidence Predictions for Unrecognizable Images.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-adversarial-attacks/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/mrzhu-cool/pix2pix-pytorch">pix2pix-pytorch</a></td>
<td align="center">PyTorch implementation of "Image-to-Image Translation Using Conditional Adversarial Networks".</td>
<td align="center"><code>PyTorch</code></td>
<td align="center">Not Found</td>
</tr>
<tr>
<td align="center"><a href="https://github.com/NVIDIA/semantic-segmentation">NVIDIA/semantic-segmentation</a></td>
<td align="center">A PyTorch Implementation of Improving Semantic Segmentation via Video Propagation and Label Relaxation, In CVPR2019.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/NVIDIA/semantic-segmentation/master/LICENSE" rel="nofollow">CC BY-NC-SA 4.0 license</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/kentsyx/Neural-IMage-Assessment">Neural-IMage-Assessment</a></td>
<td align="center">A PyTorch Implementation of Neural IMage Assessment.</td>
<td align="center"><code>PyTorch</code></td>
<td align="center">Not Found</td>
</tr>
<tr>
<td align="center"><a href="https://github.com/mlmed/torchxrayvision">torchxrayvision</a></td>
<td align="center">Pretrained models for chest X-ray (CXR) pathology predictions. Medical, Healthcare, Radiology</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/mlmed/torchxrayvision/master/LICENSE" rel="nofollow">Apache License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/rwightman/pytorch-image-models">pytorch-image-models</a></td>
<td align="center">PyTorch image models, scripts, pretrained weights -- (SE)ResNet/ResNeXT, DPN, EfficientNet, MixNet, MobileNet-V3/V2, MNASNet, Single-Path NAS, FBNet, and more</td>
<td align="center"><code>PyTorch</code></td>
<td align="center"><a href="https://github.com/rwightman/pytorch-image-models/blob/master/LICENSE">Apache License 2.0</a></td>
</tr>
</tbody>
</table>
<div align="right" dir="auto">
    <b></b><b><a href="#framework">↥ Back To Top</a></b>
</div>
<hr>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Caffe <a name="user-content-caffe"></a></h3><a id="user-content-caffe-" class="anchor-element" aria-label="永久链接：咖啡厅" href="#caffe-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<table>
<thead>
<tr>
<th align="center">Model Name</th>
<th align="center">Description</th>
<th align="center">Framework</th>
<th align="center">License</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a></td>
<td align="center">OpenPose represents the first real-time multi-person system to jointly detect human body, hand, and facial keypoints (in total 130 keypoints) on single images.</td>
<td align="center"><code>Caffe</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/LICENSE" rel="nofollow">Custom</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/shelhamer/fcn.berkeleyvision.org">Fully Convolutional Networks for Semantic Segmentation</a></td>
<td align="center">Fully Convolutional Models for Semantic Segmentation.</td>
<td align="center"><code>Caffe</code></td>
<td align="center">Not Found</td>
</tr>
<tr>
<td align="center"><a href="https://github.com/richzhang/colorization">Colorful Image Colorization</a></td>
<td align="center">Colorful Image Colorization.</td>
<td align="center"><code>Caffe</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/richzhang/colorization/master/LICENSE" rel="nofollow">BSD-2-Clause License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/YuwenXiong/py-R-FCN">R-FCN</a></td>
<td align="center">R-FCN: Object Detection via Region-based Fully Convolutional Networks.</td>
<td align="center"><code>Caffe</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/YuwenXiong/py-R-FCN/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/jcjohnson/cnn-vis">cnn-vis</a></td>
<td align="center">Inspired by Google's recent Inceptionism blog post, cnn-vis is an open-source tool that lets you use convolutional neural networks to generate images.</td>
<td align="center"><code>Caffe</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/jcjohnson/cnn-vis/master/LICENSE" rel="nofollow">The MIT License (MIT)</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/HyeonwooNoh/DeconvNet">DeconvNet</a></td>
<td align="center">Learning Deconvolution Network for Semantic Segmentation.</td>
<td align="center"><code>Caffe</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/HyeonwooNoh/DeconvNet/master/LICENSE" rel="nofollow">Custom</a></td>
</tr>
</tbody>
</table>
<div align="right" dir="auto">
    <b></b><b><a href="#framework">↥ Back To Top</a></b>
</div>
<hr>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">MXNet <a name="user-content-mxnet"></a></h3><a id="user-content-mxnet-" class="anchor-element" aria-label="永久链接：MXNet" href="#mxnet-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<table>
<thead>
<tr>
<th align="center">Model Name</th>
<th align="center">Description</th>
<th align="center">Framework</th>
<th align="center">License</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://github.com/ijkguo/mx-rcnn">Faster RCNN</a></td>
<td align="center">Region Proposal Network solves object detection as a regression problem.</td>
<td align="center"><code>MXNet</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/ijkguo/mx-rcnn/master/LICENSE" rel="nofollow">Apache License, Version 2.0</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/zhreshold/mxnet-ssd">SSD</a></td>
<td align="center">SSD is an unified framework for object detection with a single network.</td>
<td align="center"><code>MXNet</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/zhreshold/mxnet-ssd/master/LICENSE" rel="nofollow">MIT License</a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/unsky/focal-loss"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">更快的 RCNN+Focal Loss</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">该代码是密集物体检测的焦点损失的非官方版本。</font></font></td>
<td align="center"><code>MXNet</code></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">未找到</font></font></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/oyxhust/CNN-LSTM-CTC-text-recognition"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CNN-LSTM-CTC</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">我实现了三种不同的文本识别模型，它们都由CTC损失层组成，以实现文本图像的无分割。</font></font></td>
<td align="center"><code>MXNet</code></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">未找到</font></font></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/jessemelpolio/Faster_RCNN_for_DOTA"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Faster_RCNN_for_DOTA</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">这是论文</font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DOTA：航空图像中对象检测的大型数据集</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">的官方存储库。</font></font></td>
<td align="center"><code>MXNet</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/jessemelpolio/Faster_RCNN_for_DOTA/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">阿帕奇许可证</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/unsky/RetinaNet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">视网膜网</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">密集物体检测的焦点损失。</font></font></td>
<td align="center"><code>MXNet</code></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">未找到</font></font></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/liangfu/mxnet-mobilenet-v2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">移动网络V2</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">这是 MobileNetV2 架构的 MXNet 实现，如论文“</font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">反转残差和线性瓶颈：用于分类、检测和分割的移动网络”</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">中所述。</font></font></td>
<td align="center"><code>MXNet</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/liangfu/mxnet-mobilenet-v2/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">阿帕奇许可证</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/TuSimple/neuron-selectivity-transfer"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">神经元选择性转移</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">该代码是论文Like What You Like: Knowledge Distill via Neuron Selectivity Transfer</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">中 imagenet 分类实验的重新实现</font><font style="vertical-align: inherit;">。</font></font></td>
<td align="center"><code>MXNet</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/TuSimple/neuron-selectivity-transfer/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">阿帕奇许可证</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/chinakook/MobileNetV2.mxnet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">移动网络V2</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">这是 MobileNetV2 架构的 Gluon 实现，如论文“</font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">反转残差和线性瓶颈：用于分类、检测和分割的移动网络”</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">中所述。</font></font></td>
<td align="center"><code>MXNet</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/chinakook/MobileNetV2.mxnet/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">阿帕奇许可证</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/TuSimple/sparse-structure-selection"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">稀疏结构选择</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">该代码是论文《深度神经网络的数据驱动稀疏结构选择》</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">中 imagenet 分类实验的重新实现</font><font style="vertical-align: inherit;">。</font></font></td>
<td align="center"><code>MXNet</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/TuSimple/sparse-structure-selection/master/LICENSE" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">阿帕奇许可证</font></font></a></td>
</tr>
<tr>
<td align="center"><a href="https://github.com/NVIDIA/FastPhotoStyle"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">快速照片风格</font></font></a></td>
<td align="center"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">真实感图像风格化的封闭式解决方案。</font></font></td>
<td align="center"><code>MXNet</code></td>
<td align="center"><a href="https://raw.githubusercontent.com/NVIDIA/FastPhotoStyle/master/LICENSE.md" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">知识共享署名-非商业性-相同方式共享 4.0 国际公共许可证</font></font></a></td>
</tr>
</tbody>
</table>
<div align="right" dir="auto">
    <b></b><b><a href="#framework"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">↥ 返回顶部</font></font></a></b>
</div>
<hr>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">贡献</font></font></h2><a id="user-content-contributions" class="anchor-element" aria-label="永久链接：贡献" href="#contributions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">随时欢迎您的贡献！</font><font style="vertical-align: inherit;">请看一下</font></font><code>contributing.md</code></p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">执照</font></font></h2><a id="user-content-license" class="anchor-element" aria-label="永久链接：许可证" href="#license"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="/balavenkatesh3322/CV-pretrained-model/blob/master/LICENSE"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">麻省理工学院许可证</font></font></a></p>
</article></div>
